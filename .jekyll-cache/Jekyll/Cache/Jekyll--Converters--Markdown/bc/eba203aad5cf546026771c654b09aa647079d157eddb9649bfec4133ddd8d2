I"ó<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” Autoencoder ì…ë‹ˆë‹¤. ë°ì´í„° ì••ì¶•ì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ê¸° ì‹œì‘í•´ì„œ ìƒì„±, ë³€í™˜ ë“± ë‹¤ì–‘í•œ ëª©ì ì— í™œìš©ë˜ê³  ìˆëŠ” Autoencoderì˜ ì¢…ë¥˜ì— ëŒ€í•´ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤.</p>

<h2 id="autoencoder">Autoencoder</h2>
<p>AutoencoderëŠ” high-dimensional dataë¥¼ ë³´ë‹¤ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•í•˜ê¸° ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post6/post6_thumbs.jpg" alt="autoencoder" /></p>

<p>ì••ì¶•ì„ ìœ„í•œ êµ¬ì¡°ë¡œ neural networkë¥¼ ì‚¬ìš©í•˜ê³  í•™ìŠµì„ ìœ„í•œ targetì„ ì„¤ì •í•˜ê¸° ìœ„í•´
reconstructionì„ ìˆ˜í–‰í•˜ëŠ” decoder êµ¬ì¡°ë¥¼ bottleneck layer ë’¤ì— ì¶”ê°€í•˜ê²Œ ë©ë‹ˆë‹¤. 
ë°ì´í„° ì••ì¶•, ì£¼ìš” feature ì¶”ì¶œì„ ìœ„í•œ ë°©ë²•ìœ¼ë¡œ PCAê°€ ë§ì´ ì•Œë ¤ì ¸ ìˆëŠ”ë°, Autoencoderì˜ neural network êµ¬ì¡°ì—ì„œ non-linearlityë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œí•˜ëŠ” 
activation function(ex. relu, sigmoid)ë¶€ë¶„ì„ ì œì™¸í•˜ë©´ ì‹¤ì œë¡œ PCAì™€ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h2 id="denoising-autoencoder">Denoising Autoencoder</h2>
<p>Identity functionì„ í•™ìŠµí•˜ëŠ” AutoencoderëŠ” overfittingì˜ ìš°ë ¤ê°€ í¬ê¸°ì— ì œì•ˆëœ ê²ƒì´ Denoising Autoencoder ì…ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post6/dae.jpg" alt="dae" /></p>

<p>Denoising AutoencoderëŠ” inputì˜ ì„ì˜ì˜ ì˜ì—­ì„ noiseë¡œ ë°”ê¾¸ê³ , reconstructionì€ noiseì˜ ì˜ì—­ê¹Œì§€ ì›ë³¸ì²˜ëŸ¼ ë³µì›í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
ì¼ë¶€ ë…¸ë“œë¥¼ ì§€ìš°ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•˜ë©´ dropoutê³¼ ê°™ì€ ê°œë…ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ dropout paperë³´ë‹¤ 4ë…„ì´ë‚˜ ë¨¼ì € ë‚˜ì˜¨ ì—°êµ¬ë¼ê³  í•˜ë„¤ìš”.</p>

<h2 id="sparse-autoencoder">Sparse Autoencoder</h2>
<p>Sparse AutoencoderëŠ” autoencoderì˜ hidden layerì— sparsity ì¡°ê±´ì„ ì¶”ê°€í•œ ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
sparsity ì¡°ê±´ì´ë€ hidden unitì˜ activationì„ ì œí•œí•˜ëŠ” ê²ƒì¸ë°, ì´ë¥¼ í†µí•´ hidden unitì˜ ìˆ˜ê°€ ì»¤ì ¸ë„ ì¢€ ë” ì˜ë¯¸ ìˆëŠ” featureë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
Sparse Autoencoder ì¤‘ ëŒ€í‘œê²©ì¸ k-Sparse Autoencoderì˜ ê²°ê³¼ë¥¼ ë³´ê² ìŠµë‹ˆë‹¤.</p>

<p><img src="/assets/img/post6/sae.jpg" alt="sae" /></p>

<p>k-Sparse AutoencoderëŠ” hidden layerì˜ activation ê°’ ì¤‘ ê°€ì¥ í° kê°œì˜ ê°’ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br />
k ê°’ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ì ì€ ì •ë³´ë¡œ reconstructionì„ ìˆ˜í–‰í•´ì•¼ í•˜ê¸°ì— hidden layerì—ì„œëŠ” ì ì  high level featureë¥¼ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.</p>

<h2 id="convolutional-autoencoder">Convolutional Autoencoder</h2>
<p>AutoencoderëŠ” ì •í˜•, ë¹„ì •í˜•(ì–¸ì–´, ì´ë¯¸ì§€ ë“±) ë“±ì˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, ì´ë¯¸ì§€ ë°ì´í„°ì— ëŒ€í•´ Autoencoderë¥¼ ì‚¬ìš©í•  ë• 
ë³´í†µ Convolutional Autoencoderë¥¼ ê¸°ë³¸ìœ¼ë¡œ ìƒê°í•©ë‹ˆë‹¤. CAEëŠ” ì´ì „ê¹Œì§€ inputì„ flattení•´ì„œ ì‚¬ìš©í•˜ë˜ ë°©ì‹ì— CNNì„ ì¶”ê°€í•œ ê²ƒì…ë‹ˆë‹¤.
ì´ë¡œì¨ parameter ìˆ˜ë¥¼ ì¤„ì—¬ ì¢€ ë” í° ì´ë¯¸ì§€ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , convolution ë‹¨ìœ„ë¡œ ì¢€ ë” ì˜ë¯¸ìˆëŠ” featureë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì  ë“± CNNì˜ ê¸°ë³¸ì ì¸ ì¥ì ì„ ì–»ê²Œë˜ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="variational-autoencoder">Variational Autoencoder</h2>
<p><img src="/assets/img/post6/vae.jpg" alt="vae" /></p>

<p>Variational AutoencoderëŠ” Autoencoder êµ¬ì¡°ì—ì„œ latent variable zë¥¼ ì–»ê¸°ìœ„í•œ trickê³¼ lossì— KL divergenceê°€ ì¶”ê°€ëœ ê²ƒì¸ë°ìš”. 
ì´ë ‡ê²Œ í‘œë©´ì ìœ¼ë¡œëŠ” Autoencoderì— ì¼ë¶€ ê°œë…ì´ ì¶”ê°€ëœ ê²ƒìœ¼ë¡œ ë³´ì´ì§€ë§Œ ëª©ì  ìì²´ê°€ Autoencoderì™€ëŠ” ë‹¤ë¥´ê¸°ì— ì „í˜€ ë‹¤ë¥¸ ëª¨ë¸ë¡œ êµ¬ë¶„ë˜ê³¤ í•©ë‹ˆë‹¤.
ì•ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒì²˜ëŸ¼ AutoencoderëŠ” ë°ì´í„° ì••ì¶•, ì¦‰ íŠ¹ì • maniflodë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ ëª¨ë¸ì¸ ë°˜ë©´, VAEëŠ” GANì²˜ëŸ¼ Generative modelë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post6/vae_loss.jpg" alt="vae_loss" /></p>

<p>VAEì˜ LossëŠ” ìœ„ì™€ ê°™ì´ ì •ì˜ë˜ê³  ì´ë¥¼ ìœ ë„í•  ë•Œ ë³´í†µì€ log likelihoodë¥¼ maximize í•˜ëŠ” ì‹ì—ì„œ ì¶œë°œí•˜ëŠ” ë°, 
ê°œì¸ì ìœ¼ë¡œ ì´ìƒì ì¸ sampling í•¨ìˆ˜ì— ì‹¤ì œ sampling í•¨ìˆ˜ë¥¼ ê·¼ì‚¬ì‹œí‚¤ëŠ” termì—ì„œ ì¶œë°œí•˜ëŠ” ê²ƒì´ ì¢€ ë”</p>

<p>ìœ„ Loss</p>

<p>&lt; â€”â€”â€”â€”â€”â€”â€”â€” ì‘ì„±ì¤‘ â€”â€”â€”â€”â€”â€”â€”â€”&gt;
<!-- latent variable zë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ normal distributionìœ¼ë¡œ ê°€ì •í•˜ê³  encoderì˜ outputì„ zì— ê·¼ì‚¬ì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
ì´ ë°©ë²•ìœ¼ë¡œ ì¸í•´ VAEëŠ” ë‹¨ìˆœíˆ inputì„ ì™¸ìš¸ ìˆ˜ ì—†ê²Œë˜ê³  latent variableì˜ ìˆ˜ê°€ ë³€í•´ë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤. --></p>

<h2 id="reference">Reference</h2>
<p><a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">Lilâ€™log : From Autoencoder to Beta-VAE</a><br />
<a href="https://blog.naver.com/laonple/220943887634">ë¼ì˜¨í”¼í”Œ : ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë°©ë²•(part 14) - AutoEncoder(5)</a><br />
<a href="https://blog.naver.com/laonple/220949087243">ë¼ì˜¨í”¼í”Œ : ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë°©ë²•(part 14) - AutoEncoder(6)</a><br />
<a href="https://www.jeremyjordan.me/variational-autoencoders/">Jeremy Jordan : Variational autoencoders</a></p>

<!-- paper :  
<a href="https://arxiv.org/abs/2006.07733â€‹">Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</a>

etc :   
<a href="https://hoya012.github.io/blog/byol/">HOYA012'S RESEARCH BLOG : Bootstrap Your Own Latentï¼š A New Approach to Self-Supervised Learning ë¦¬ë·°</a>  
<a href="https://2-chae.github.io/category/2.papers/26">https://2-chae.github.io/category/2.papers/26</a>  
<a href="https://cool24151.tistory.com/85">https://cool24151.tistory.com/85</a> 
<a href="https://www.youtube.com/watch?v=BuyWUSPJicM">ë”¥ëŸ¬ë‹ë…¼ë¬¸ì½ê¸°ëª¨ì„ : ì¡°ìš©ë¯¼ - Bootstrap Your Own Latent(BYOL)</a>  -->

:ET