I"v<p>이번 논문은 Image Representation Learning에서 SimCLR과 MoCo의 성능을 모두 뛰어넘었다고 하는 BYOL 입니다. Google DeepMind와 Imperial College에서 발표하였고, 기존 연구들에 비해 Supervised Learning 성능(ResNet50, ImageNet acc 기준)에 가장 근접한 결과를 보여주고 있습니다.</p>

<h2 id="intro">Intro</h2>
<p>Image representation learning 및 Unsupervised learning 분야에서 MoCo, SimCLR 등이 높은 성능을 보여주었지만, 이러한 선행 연구들에선 공통적으로, 적절한 augmentation의 사용과 학습 과정에서 많은 negative sample을 보는 것이 중요했습니다. 각 연구들에선 ImageNet data에 대해 실험을 통해 적절한 Augmentation과 batch size를 설정했지만, 이 값들은 data domain에 따라 달라질 수 있는 값들입니다. 즉, custom data에 적용할 경우, 대량의 Unlabeled data와 소량의 Labeled data를 사용한 Semi-Supervised Learning을 적용한다고 생각하면, 적절한 Augmentation 선정과 사용가능한 resource 내에서 batch size의 설정 등이 많은 실험을 필요로 할 것입니다.</p>

<p>이 논문에선 이전 연구들에 비해 batch size와 augmentation에 대한 robustness를 증가시키면서, Image representation learning 성능도 개선된 결과를 보여줍니다. 특히, 학습 과정에서 negative sample을 전혀 사용하지 않았다는 점이 이전 연구와는 가장 큰 차이라고 할 수 있습니다. 두 개의 neural network을 사용하고 이 중 하나를 target network로 사용하는 것이 핵심 아이디어 입니다.</p>

<h2 id="methods">Methods</h2>
<p>논문의 motivation은 간단한 실험에서 출발합니다.</p>

<p><img src="/assets/img/post5/motive.jpg" alt="motive" /></p>

<ul>
  <li>step1 : randomly initialized network를 freeze하고 linear evaluation을 진행합니다.<br />
-&gt; Acc 1.4%</li>
  <li>step2 : step1의 network에 MLP를 붙여 prediction 값을 얻습니다.</li>
  <li>step3 : step2에서 얻은 prediction 값을 target으로 하여 step1처럼 randomly initialized network를 학습하고 linear evaluation을 진행합니다. <br />
-&gt; Acc 18.8%</li>
</ul>

<p>“동일 instance에 대해 다른 augmentation을 적용한 이미지는 representation이 동일해야 한다.” 가 instance discrimination 방식인 contrastive learning의 기본 아이디어 였는데요. 이 아이디어만으로 학습 framework을 구성할 시, 모델은 input과 무관하게 동일 representation을 출력하도록 학습될 수 있고 이를 논문에서는 ‘collapsed representations’이라고 말합니다. 이 문제가 이전 연구들에서 negative sample을 사용해야만 했던 이유라고 할 수 있습니다.</p>

<p>여기서, 학습을 시작하기전 randomly initialized network에서 얻은 representation을 생각해보면 이미지의 특성을 잘 반영한 representation은 아니겠지만, 아직 학습을 시작하지 않았기에 collapsed 문제는 없을 것으로 생각할 수 있습니다. 이렇게 얻은 representation의 성능이 step1에서 구한 1.4%였습니다. ImageNet 데이터가 1000개의 class임을 감안하면 학습한 1개의 linear layer에서 어느정도의 학습은 이루어진 것으로 보입니다.</p>

<p>step2~3의 결과는 step1에서 얻은 의미없는(randomly initilized 이므로) representation을 target으로 학습하는 것도 어느 정도의 의미는 있다는 것을 보여줍니다. 이 부분이 개인적으로 좀 신기한 부분이었는데요. 틀린 답으로 학습하는 것도 전혀 학습하지 않은 것보다는 도움이 된다고 해석할 수 있기 때문입니다.</p>

<p>논문에선 위 실험결과를 시작으로 representation을 학습할 online network와 학습하는 prediction을 출력할 target network를 아래와 같이 설계하게 됩니다.</p>

<p><img src="/assets/img/post5/byol_thumbs.jpg" alt="byol_thumbs" /></p>

<ul>
  <li><img src="https://latex.codecogs.com/svg.latex?\; v, v' : " height="20" /> augmented image</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; f_{\theta}, f_{\xi} :" wdith="20" /> randomly initialized network</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; g_{\theta}, g_{\xi} :" wdith="20" /> projector</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; q_{\theta} : " height="13" /> predictor</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; sg(z'_{\xi}) : " height="20" /> stop gradient</li>
</ul>

<p>서로 다른 augmentation을 적용한 이미지의 feature를 각각 계산하여 loss를 구하고 downstream task에는 <img src="https://latex.codecogs.com/svg.latex?\; y_{\theta}" />만 사용되는 점은 SimCLR와 유사한 형태이고, online과 target network가 구분된다는 점, online에만 predictor가 추가적으로 사용한 점이 큰 차이라고 볼 수 있습니다.</p>

<p>target network의 output은 stop gradient를 적용하고 loss를 계산하기에 online network만 학습 과정 중 직접적으로 학습되고, target network는 online network의 exponential moving average로 매 epoch 마다 업데이트 됩니다. 이 부분은 MoCo에서 사용한 momentum encoder와 유사해 보입니다.</p>

<p><img src="https://latex.codecogs.com/svg.latex?\; \xi \leftarrow \tau \xi + (1-\tau)\theta " height="25" /></p>

<p>Loss와 전체 학습 알고리즘은 아래와 같은 모양이고, loss에 사용되는 <img src="https://latex.codecogs.com/svg.latex?\; q_{\theta}(z_{\theta}), z'_{\xi} " />는 l2 normalization되어 사용됩니다. 그리고 online과 target network가 비대칭적인 구조이기에 augmentation을 바꿔서 적용한 loss도 더하여 최종 loss가 계산됩니다.</p>

<p><img src="/assets/img/post5/loss.jpg" alt="loss" />
<img src="/assets/img/post5/algorithm.jpg" alt="algorithm" /></p>

<p>사용된 augmentation, architecture, optimization 방식은 대부분 SimCLR과 비슷하다고 하고, 논문 Appendix에 자세히 정리되어 있습니다.</p>

<h3 id="augmentation">Augmentation</h3>
<ul>
  <li>random patch, resize, random horizontal flip</li>
  <li>color distortion</li>
  <li>Gaussian blur, solarization</li>
</ul>

<h3 id="architecture">Architecture</h3>
<ul>
  <li>ResNet(layer:50,101,152,200, width:1x~4x)</li>
  <li>projector, predictor : MLP(4096, BN, ReLU, 256), 두 번째 layer 뒤에 BN이 사용되지 않은 점이 SimCLR과 다름</li>
</ul>

<h3 id="optimization">Optimization</h3>
<ul>
  <li>LARS optimizer, cosine decay lr schedule, 1000 epochs</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; \tau_{base}=0.996  " height="20" />, <img src="/assets/img/post5/tau.jpg" alt="tau" /></li>
  <li>ResNet50 : batch size=4096, 512 Clout TPU v3 cores, 8 hours</li>
</ul>

<h2 id="results">Results</h2>
<p>아래 ImageNet 데이터에 대한 Image representation 성능 측정에는 Unsupervised Learning 성능 측정에 표준으로 사용되는 Linear e</p>

<h2 id="conclusion">Conclusion</h2>

<p>&lt;—– 작성중 —–&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/2006.07733​">Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</a></p>

<p>etc : <br />
<a href="https://hoya012.github.io/blog/byol/">HOYA012’S RESEARCH BLOG : Bootstrap Your Own Latent： A New Approach to Self-Supervised Learning 리뷰</a><br />
<a href="https://2-chae.github.io/category/2.papers/26">https://2-chae.github.io/category/2.papers/26</a><br />
<a href="https://cool24151.tistory.com/85">https://cool24151.tistory.com/85</a> 
<a href="https://www.youtube.com/watch?v=BuyWUSPJicM">딥러닝논문읽기모임 : 조용민 - Bootstrap Your Own Latent(BYOL)</a></p>

:ET