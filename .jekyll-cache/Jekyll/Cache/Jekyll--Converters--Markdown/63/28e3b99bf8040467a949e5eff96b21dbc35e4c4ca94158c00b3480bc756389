I"D<p>이 논문은 Facebook AI에서 발표하였고, 이미지 Representation Learning 분야에서 SimCLR과 라이벌 격인 논문입니다. SimCLR처럼 Contrastive Learning을 사용하였으며 momentum encoder를 사용하는 부분이 가장 큰 차이라고 할 수 있을 것 같습니다.</p>

<h2 id="intro">Intro</h2>
<p>NLP에서는 GPT, BERT에서 보여주었듯이 Unsupervised Learning이 탁월한 성과를 보여주었지만, Computer vision 분야에서는 여전히 Supervised Learning으로 pre-trained 된 모델의 성능이 우세했습니다. 하지만 MoCo를 활용하여 representation을 학습한 모델은 segmentation, detection을 포함한 7가지 downstream task에서 ImageNet pretrained 모델 대비 우수한 성능을 보여주었습니다. 최근 이미지 representation 학습에서 뛰어난 성과를 보여준 contrastive learning과 dynamic dictionary를 활용하였다고 하는데, 각각에 대해 자세히 살펴보겠습니다.</p>

<h2 id="methode">Methode</h2>
<p><img src="/assets/img/post4/loss_function.jpg" alt="loss" /></p>
<ul>
  <li><img src="https://latex.codecogs.com/svg.latex?\; L_{q}" /> : query 하나에 대한 loss</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; \tau" /> : temperature, hyper parameter</li>
</ul>

<p>사용된 loss는 위와 같은 모양입니다.<br />
distanace로는 dot product가 사용되었고, sample 하나 당 1개의 positive sample과는 가깝게, K개의 negative sample과는 멀게하는 InfoNCE가 사용되었습니다.</p>

<p><img src="/assets/img/post4/figure1.jpg" alt="figure1" /></p>

<p>위 그림에서 queue가 dictionary를 의미하며, batch size보다 dictionary size를 크게하는 것이 학습 시 좀 더 많은 negative sample을 볼 수 있게 합니다. input 이미지가 momentum encoder를 통과한 key값이 queue를 구성하게 되는데, 그림의 구조가 batch마다 반복되는 것은 아니고, 한 번 queue에 저장된 key값은 추가되는 key값이 dictionary size를 초과하지 않는 한 유지되는 형태입니다.<br />
학습</p>

<h2 id="results">Results</h2>

<h2 id="conclusion">Conclusion</h2>

<p>&lt;———- ToDo ———-&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/1911.05722​">Momentum Contrast for Unsupervised Visual Representation Learning</a><br />
<a href="https://arxiv.org/abs/2003.04297​​">Improved Baselines with Momentum Contrastive Learning</a></p>

<p>etc : <br />
<a href="https://www.youtube.com/watch?v=2Undxq7jlsA&amp;t=383s">PR-260: Momentum Contrast for Unsupervised Visual Representation Learning</a><br />
<a href="https://velog.io/@tobigs-gm1/Self-Supervised-Learning">투빅스 생성모델 세미나 : Self-Supervised Learning</a><br />
<a href="https://cool24151.tistory.com/82">https://cool24151.tistory.com/82</a></p>

:ET