I"ä<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„  ì§ì „ NeRF ë‚´ìš©ì— ì´ì–´ NeRFì™€ ê´€ë ¨ëœ í›„ì† ì—°êµ¬ë“¤ì—ì„œ ëŒ€í•´ì„œ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤.</p>

<h2 id="baking-neural-radiance-fields-for-real-time-view-synthesis">Baking Neural Radiance Fields for Real-Time View Synthesis</h2>
<p>í•™ìŠµëœ NeRFë¥¼ ì‚¬ìš©í•´ì„œ viewë¥¼ ë Œë”ë§ í•˜ë ¤ë©´ rayë³„ë¡œ MLPë¥¼ íƒ€ì•¼í•˜ë¯€ë¡œ MLPë¥¼ ìˆ˜ë°±ë²ˆ í†µê³¼í•´ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Sparse Neural Radiance Grid(SNeRG)ì—ì„œëŠ” í•™ìŠµ ì¤‘ì— MLPì˜ outputì„ ë¯¸ë¦¬ ì €ì¥(â€œbakeâ€í•œë‹¤ê³  í•¨)í•˜ëŠ” êµ¬ì¡°ë¥¼ ì œì•ˆí–ˆê³ , ì´ë¥¼ í†µí•´ ì¼ë°˜ì ì¸ í•˜ë“œì›¨ì–´ì—ì„œ real-time renderingì´ ê°€ëŠ¥í•˜ê²Œ ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig7.png" /></p>

<p>(*NeRFì—ì„œ densityë¼ê³  í–ˆëŠ”ë° ì´ ë…¼ë¬¸ì—ì„  opacityë¡œ í˜¼ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. â€˜ì§„í•œì •ë„â€™ë¥¼ â€˜íˆ¬ëª…í•œì •ë„â€™ë¡œ ë°”ê¿”ì„œ í‘œí˜„í•œ ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤.)</p>

<p>ìœ„ ê·¸ë¦¼ ì¤‘ Alpha-compositeì€ ray ìœ„ì— ìˆëŠ” colorë“¤ì„ weighted sumí•˜ëŠ” ê³¼ì •ì„ ë§í•˜ê³ , ì „ì²´ì ì¸ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ol>
  <li>trained NeRFë¥¼ í†µí•´ sparseí•œ ì˜ˆì¸¡ê°’ë“¤ì„ 3D voxel gridì— ì €ì¥í•´ ë‘”ë‹¤.</li>
  <li>ê·¸ëŸ¬ë©´ voxelì—” color, opacity, view dependentí•œ feature vectorë“¤ì´ ì €ì¥ë  ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë¦¬ê³  test ì‹œ, renderingí•  ë• voxelì— ì €ì¥ëœ colorì™€ feature vectorë“¤ì„ weighted sumí•˜ê²Œ ë˜ëŠ”ë°, feature vectorê°€ view dependentí•˜ë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ ë³„ë„ì˜ viewing directionê³¼ í•¨ê»˜ NNì„ ì¶”ê°€ë¡œ ì‚¬ìš©í•´ specular color ì •ë³´ë¥¼ êµ¬í•œë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ìµœì¢… outputì€ voxelì— ì €ì¥ë˜ì–´ ìˆë˜ colorì˜ weighted sumê³¼ specular colorì˜ í•©ìœ¼ë¡œ êµ¬í•œë‹¤.</li>
</ol>

<p>ê¸°ì¡´ NeRFì—ì„  ray ìœ„ì— ìˆëŠ” 3d pointë§ˆë‹¤ NNì„ í†µê³¼í•´ì•¼ í–ˆëŠ”ë°, SNeRGì—ì„  outputì´ ë  2d ì´ë¯¸ì§€ì˜ í”½ì…€ë§ˆë‹¤ ì¶”ê°€ë¡œ ì‚¬ìš©ëœ tiny NNë§Œ í†µê³¼í•˜ë©´ ë˜ê¸°ì— rendering ì†ë„ê°€ ë¹¨ë¼ì§€ê²Œ ë©ë‹ˆë‹¤.</p>

<h2 id="fastnerf--high-fidelity-neural-rendering-at-200fps">FastNeRF : High-Fidelity Neural Rendering at 200FPS</h2>

<p><img src="/assets/img/post9/fig8.png" />
ì´ ë…¼ë¬¸ì—ì„  positionê³¼ directionì„ ê°ê° inputìœ¼ë¡œ í•˜ëŠ” neural network ë‘ ê°œë¥¼ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. positionê³¼ directionì— ê°ê° k, lê°œì˜ ê°’ì´ ì‚¬ìš©ëœë‹¤ê³  í•  ë•Œ, NeRFì˜ memory complexityëŠ” $O(k^3 l^2)$ì´ì—ˆëŠ”ë°, FastNeRFì—ì„  $O(k^3 \times (1+3D)+ l^2\times D)$ë¡œ ì¤„ì–´ ë“¤ê²Œ ë©ë‹ˆë‹¤.</p>

<h2 id="kilonerf--speeding-up-neural-radiance-fields">KiloNeRF : Speeding Up Neural Radiance Fields</h2>

<p><img src="/assets/img/post9/fig9.png" /></p>

<p>NeRFì²˜ëŸ¼ ì „ì²´ sceneì„ í•˜ë‚˜ì˜ í° MLPë¡œ representí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ scene ì¼ë¶€ë¥¼ representí•  ìˆ˜ ìˆëŠ” ì‘ì€ MLPë¥¼ ìˆ˜ì²œê°œë¥¼ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig10.png" /></p>

<p>tiny NNì„ ì‚¬ìš©í•´ì„œ sceneì˜ ì¼ë¶€ë¥¼ ìƒì„±í•˜ê³  í•©ì¹˜ë©´, ìœ„ (a)ì²˜ëŸ¼ ì´ìƒí•œ ë¬¼ì²´ë“¤ì´ ìƒê¸°ëŠ” ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ original nerfë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ teacherë¡œ í•˜ì—¬ knowledge distillationë°©ì‹ìœ¼ë¡œ fine tuningì„ ì§„í–‰í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. distillationì€ teacherì˜ outputì¸ colorì™€ densityì— ë§¤ì¹­í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆê³ , ì´ë ‡ê²Œ í•˜ì (b)ì²˜ëŸ¼ artifactsë¥¼ ì œê±°í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>

<h2 id="learned-initializations-for-optimizing-coordinate-based-neural-representations">Learned Initializations for Optimizing Coordinate-Based Neural Representations</h2>

<p><img src="/assets/img/post9/fig11.png" /></p>

<p>ì¢Œí‘œë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ rgbë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì€ ë™ì¼í•œ ë¬¼ì²´ì— ëŒ€í•´ ë‹¤ì–‘í•œ sceneì´ ì¡´ì¬í•  ë•Œ, ì´ ì¤‘ ì¼ë¶€ë¥¼ í•™ìŠµí•˜ì—¬ í•™ìŠµí•˜ì§€ ì•Šì€ sceneì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì¦‰, novel viewë¥¼ ìƒì„±í•˜ê³  ì‹¶ì€ ë¬¼ì²´ë§ˆë‹¤ from scratchë¡œ í•™ìŠµí•´ì•¼ í•˜ëŠ”ë°, ì´ê²Œ ë„ˆë¬´ ë¹„íš¨ìœ¨ì ì´ë‹ˆ, initial weights parameterë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ meta-learning algorithmì„ í™œìš©í•œë‹¤ëŠ” ì»¨ì…‰ì˜ ë…¼ë¬¸ì…ë‹ˆë‹¤. meta-learningì˜ ëŒ€í‘œì ì¸ ë…¼ë¬¸ì¸ MAMLê³¼ Reptile ë°©ì‹ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.</p>

<h2 id="pixelnerf--neural-radiance-fields-from-one-or-few-images">PixelNeRF : Neural Radiance Fields from One or Few Images</h2>

<p>ê¸°ì¡´ì—ëŠ” ê°œë³„ sceneë“¤ì´ ê°ê° optimize ë¬ëŠ”ë°, PixelNeRFì—ì„  input viewë¥¼ ì…ë ¥ìœ¼ë¡œí•˜ì—¬ novel viewë¥¼ ìƒì„±í•˜ëŠ” í•™ìŠµ ê°œë…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì ì€ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•´ë„ novel viewë¥¼ ìƒì„±í•  ìˆ˜ ìˆê³ , novel viewë¥¼ ìƒì„±í•˜ëŠ” ë° multi viewì˜ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” êµ¬ì¡°ë„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig12.png" /></p>

<p>ìœ„ ê·¸ë¦¼ì˜ ìœ„, ì•„ë˜ëŠ” ê°ê° 1 ì¥, 3 ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ì„œ novel viewë¥¼ ìƒì„±í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì ì€ ë°ì´í„°ë§Œ ì‚¬ìš©í–ˆìŒì—ë„ PixelNeRFëŠ” NeRFì— ë¹„í•´ novel viewë¥¼ ì˜ ìƒì„±í•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig13.png" /></p>

<p>ìœ„ëŠ” ì´ë¯¸ì§€ í•œì¥(single view)ìœ¼ë¡œ novel viewë¥¼ ìƒì„±í•˜ëŠ” ë„ì‹ ì…ë‹ˆë‹¤.</p>

<p>inputì´ë¯¸ì§€ê°€ CNN Encoderë¥¼ í†µê³¼í•˜ì—¬ feature volume Wë¥¼ êµ¬í•˜ê³  target viewë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ray ìœ„ì— ìˆëŠ”  3d point x, viewing direction dë¥¼ ê°™ì´ ì‚¬ìš©í•˜ì—¬ $RGB\sigma$ë¥¼ êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. weighted sumìœ¼ë¡œ predicted target viewë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ NeRFì™€ ë™ì¼í•©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig14.png" /></p>

<p>multi-viewë¡œ novel viewë¥¼ ìƒì„±í•˜ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤.</p>

<p>single viewê·¸ë¦¼ì— ìˆëŠ” feature volume Wë¥¼ í•©ì¹˜ëŠ” ë¶€ë¶„ì´ viewë§ˆë‹¤ ì§„í–‰ë˜ê³ , í‰ê· í•œë‹¤ìŒ ì¶”ê°€ë¡œ fc layerë¥¼ í†µê³¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. $\gamma$ëŠ” sin, cosë¥¼ ì‚¬ìš©í•´ì„œ input dimensionì„ ëŠ˜ë ¤ì£¼ëŠ” positional encodingì„ ë§í•©ë‹ˆë‹¤.</p>

<h2 id="nerf-in-detail--learning-to-sample-for-view-synthesis">NeRF in Detail : Learning to Sample for View Synthesis</h2>

<p><img src="/assets/img/post9/fig15.png" /></p>

<p>Original NeRFì—ì„œ ì‚¬ìš©í–ˆë˜ coarse-to-fine mechanismì€ coarse networkì—ì„œ ì˜ˆì¸¡í•œ weightë¥¼ ì‚¬ìš©í•´ samplingí•˜ëŠ” Heuristic proposerë°©ì‹ì´ì—ˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ NeRF-IDì—ì„  Learnt proposer ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post9/fig16.png" /></p>

<p>proposerë¥¼ í•™ìŠµí•˜ëŠ” êµ¬ì¡°ë¡œ ë…¼ë¬¸ì—ì„  ìœ„ ì„¸ ê°€ì§€ë¥¼ ì œì•ˆí•˜ê³ , ê°ê°ì— ëŒ€í•œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤. (a)ëŠ” transformerêµ¬ì¡°ë¥¼ í™œìš©í•œ ê²ƒì´ê³ , (b)ëŠ” ì¼ë°˜ì ì¸ fc layerì™€ pooling êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ ê·¸ë¦¼ ì…ë‹ˆë‹¤. (c)ì—ì„œëŠ” MLPì˜ channelê³¼ tokenì„ ê°ê° mixingí•˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ëŠ” mlpmixer êµ¬ì¡°ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. (c)ì˜ ì„±ëŠ¥ì´ ë¹„êµì  ì•ˆì •ì ì´ë¼ NeRF-IDì—ì„  ì´ë¥¼ ê¸°ì¤€ proposerë¡œ ì‚¬ìš©í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h2 id="nerf-in-the-wild--neural-radiance-fields-for-unconstrained-photo-collections">NeRF in the Wild : Neural Radiance Fields for Unconstrained Photo Collections</h2>

<p>NeRFëŠ” controlled settingì—ì„œ ì˜ ë™ì‘í•˜ëŠ”ë°, NeRF-Wì—ì„  real-worldì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ë¥¼ í™•ì¥í–ˆë‹¤.</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><br />
<a href="https://arxiv.org/abs/2006.10739">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</a></p>

<p>other :<br />
<a href="https://www.youtube.com/watch?v=zkeh7Tt9tYQ">PR-302: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><br />
<a href="https://www.youtube.com/watch?v=FSG5bCkNWWo">[Seminar] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><br />
<a href="https://www.youtube.com/watch?v=SbLiGiHJ_9Q&amp;list=PLCNc54m6eBRVqlv07SMzSyMjPDB_lNXMC&amp;index=11&amp;t=5523s">Lecture8 AAA738 SeungryongKim</a></p>

:ET