I"Y<p>이 논문은 Facebook AI에서 발표하였고, 이미지 Representation Learning 분야에서 SimCLR과 라이벌 격인 논문입니다. SimCLR처럼 Contrastive Learning을 사용하였으며 momentum encoder를 사용하는 부분이 가장 큰 차이라고 할 수 있을 것 같습니다.</p>

<h2 id="intro">Intro</h2>
<p>NLP에서는 GPT, BERT에서 보여주었듯이 Unsupervised Learning이 탁월한 성과를 보여주었지만, Computer vision 분야에서는 여전히 Supervised Learning으로 pre-trained 된 모델의 성능이 우세했습니다. 하지만 MoCo를 활용하여 representation을 학습한 모델은 segmentation, detection을 포함한 7가지 downstream task에서 ImageNet pretrained 모델 대비 우수한 성능을 보여주었습니다. 최근 이미지 representation 학습에서 뛰어난 성과를 보여준 contrastive learning과 dynamic dictionary를 활용하였다고 하는데, 각각에 대해 자세히 살펴보겠습니다.</p>

<h2 id="methode">Methode</h2>
<p><img src="/assets/img/post4/loss_function.jpg" alt="loss" /></p>
<ul>
  <li><img src="https://latex.codecogs.com/svg.latex?\; L_{q}" /> : query 하나에 대한 loss</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; \tau" /> : temperature, hyper
먼저, contrative loss의 식은 위와 같은 모양 입니다.</li>
</ul>

<h2 id="results">Results</h2>

<h2 id="conclusion">Conclusion</h2>

<p>&lt;———- ToDo ———-&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/1911.05722​">Momentum Contrast for Unsupervised Visual Representation Learning</a><br />
<a href="https://arxiv.org/abs/2003.04297​​">Improved Baselines with Momentum Contrastive Learning</a></p>

<p>etc : <br />
<a href="https://www.youtube.com/watch?v=2Undxq7jlsA&amp;t=383s">PR-260: Momentum Contrast for Unsupervised Visual Representation Learning</a><br />
<a href="https://velog.io/@tobigs-gm1/Self-Supervised-Learning">투빅스 생성모델 세미나 : Self-Supervised Learning</a><br />
<a href="https://cool24151.tistory.com/82">https://cool24151.tistory.com/82</a></p>

:ET