I"]<p>이 논문은 Facebook AI에서 발표하였고, 이미지 Representation Learning 분야에서 SimCLR과 라이벌 격인 논문입니다. SimCLR처럼 Contrastive Learning을 사용하였으며 momentum encoder를 사용하는 부분이 가장 큰 차이라고 할 수 있을 것 같습니다.</p>

<h2 id="intro">Intro</h2>
<p>NLP에서는 GPT, BERT에서 보여주었듯이 Unsupervised Learning이 탁월한 성과를 보여주었지만, Computer vision 분야에서는 여전히 Supervised Learning으로 pre-trained 된 모델의 성능이 우세했습니다. 하지만 MoCo를 활용하여 representation을 학습한 모델은 segmentation, detection을 포함한 7가지 downstream task에서 ImageNet pretrained 모델 대비 우수한 성능을 보여주었습니다. 최근 이미지 representation 학습에서 뛰어난 성과를 보여준 contrastive learning과 dynamic dictionary를 활용하였다고 하는데, 각각에 대해 자세히 살펴보겠습니다.</p>

<p>&lt;———- ToDo ———-&gt;</p>

<h2 id="reference">Reference</h2>
<!-- L1 Introduction -- CS294-158-SP20 Deep Unsupervised Learning -- UC Berkeley, Spring 2020
* <a href="https://www.youtube.com/watch?v=V9Roouqfu-M">유튜브 강의</a> 
* <a href="https://sites.google.com/view/berkeley-cs294-158-sp19/home?fbclid=IwAR3EYrnDoHv05sL6Exk77urKYJ3VOs85y1UggUvKbnCBDMygcUHXL0qD-28">강의자료</a>  


Title_img : <a href="https://quoracreative.com/article/machine-learning-marketing-Sales">https://quoracreative.com/article/machine-learning-marketing-Sales</a>   -->

<p>https://www.youtube.com/watch?v=2Undxq7jlsA&amp;t=383s</p>

:ET