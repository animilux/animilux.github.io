I"8<p>이 논문은 Geoffrey Hinton 교수님이 교신저자로 참여하신 논문인데요, 학습과정에서 augmentation과의 차이를 학습하는 것으로 image representation 성능을 크게 향상 시킨 논문입니다.</p>

<h2 id="intro">Intro</h2>
<p>이 논문에선 contrastive self-supervised learning을 제안하였는데, 이 구조는 복잡하지 않으며 별도의 memory bank가 필요하지 않다는 데에서 이전 연구들과 차이가 있습니다.
논문에서 입증한 바는 크게 3가지 입니다.</p>
<ul>
  <li>data augmentation 조합이 SimCLR 구조에서 중요한 역할을 한다.</li>
  <li>representation과 contrasitve loss 사이의 non-linear transformation이 representation 성능을 향상시킨다.</li>
  <li>contrastive learning은 batch size와 training step이 클 때 효과적이다.</li>
</ul>

<p>이미지에 대한 representation learning은 크게 generative와 discriminative로 나눠집니다.
generative 접근방식은 말 그대로 이미지를 reproducing하는 과정에서 유의미한 representation을 학습하는 방식이고,
discriminative는 pretext를 정의하고 이에 맞는 objective function을 정의하는 방식입니다.</p>

<p>이 논문에선 discriminative 방법 중 하나인 contrastive learning을 사용하였고, 다양한 실험을 통해 효율적으로 representation을 학습할 수 있는 구조를 제안하고 있습니다.</p>

<h2 id="simclr--training">SimCLR &amp; Training</h2>
<p><img src="/assets/img/post2/algorithm.jpg" alt="algorithm" />
학습 알고리즘과 contrastive learning은 위 그림으로 모두 설명할 수 있습니다.<br />
먼저, 오른쪽 그림에서 <img src="https://latex.codecogs.com/svg.latex?\; \tau" width="15" />는 사전에 정의한 augmentation의 종류를 의미합니다.
input 이미지 x에 대해 random으로 선정된 두 개의 augmentation으로 <img src="https://latex.codecogs.com/svg.latex?\; \widetilde{x}_i, \widetilde{x}_j" />이 생성됩니다.
두 이미지는 f함수를 통과하여 h라는 representation으로 인코딩되고 이 값이 이미지의 representation이 되는데, f에는 resnet50의 output에서 global average pooling 된 값이 사용됩니다.
이 다음에 오는 prejection head g가 contrastive loss와 representation 사이의 non-linear transfo</p>

<p>&lt;———————- 작성중 ———————-&gt;</p>
<h2 id="reference">reference</h2>
<p>paper :<br />
SimCLR : <a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a></p>

<p>etc :<br />
The Illustrated SimCLR Framework : <a href="https://amitness.com/2020/03/illustrated-simclr/">https://amitness.com/2020/03/illustrated-simclr/</a><br />
PR-231 : <a href="https://www.youtube.com/watch?v=FWhM3juUM6s&amp;t=1s">https://www.youtube.com/watch?v=FWhM3juUM6s&amp;t=1s</a></p>

:ET