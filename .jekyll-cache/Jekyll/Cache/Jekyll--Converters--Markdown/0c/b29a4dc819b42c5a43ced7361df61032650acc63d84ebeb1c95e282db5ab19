I"Ù<p>ì´ë²ˆ ë…¼ë¬¸ì€ Image Representation Learningì—ì„œ SimCLRê³¼ MoCoì˜ ì„±ëŠ¥ì„ ëª¨ë‘ ë›°ì–´ë„˜ì—ˆë‹¤ê³  í•˜ëŠ” BYOL ì…ë‹ˆë‹¤. Google DeepMindì™€ Imperial Collegeì—ì„œ ë°œí‘œí•˜ì˜€ê³ , ê¸°ì¡´ ì—°êµ¬ë“¤ì— ë¹„í•´ Supervised Learning ì„±ëŠ¥(ResNet50, ImageNet acc ê¸°ì¤€)ì— ê°€ì¥ ê·¼ì ‘í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="intro">Intro</h2>
<p>Image representation learning ë° Unsupervised learning ë¶„ì•¼ì—ì„œ MoCo, SimCLR ë“±ì´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì´ëŸ¬í•œ ì„ í–‰ ì—°êµ¬ë“¤ì—ì„  ê³µí†µì ìœ¼ë¡œ, ì ì ˆí•œ augmentationì˜ ì‚¬ìš©ê³¼ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ negative sampleì„ ë³´ëŠ” ê²ƒì´ ì¤‘ìš”í–ˆìŠµë‹ˆë‹¤. ê° ì—°êµ¬ë“¤ì—ì„  ImageNet dataì— ëŒ€í•´ ì‹¤í—˜ì„ í†µí•´ ì ì ˆí•œ Augmentationê³¼ batch sizeë¥¼ ì„¤ì •í–ˆì§€ë§Œ, ì´ ê°’ë“¤ì€ data domainì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ê°’ë“¤ì…ë‹ˆë‹¤. ì¦‰, custom dataì— ì ìš©í•  ê²½ìš°, ëŒ€ëŸ‰ì˜ Unlabeled dataì™€ ì†ŒëŸ‰ì˜ Labeled dataë¥¼ ì‚¬ìš©í•œ Semi-Supervised Learningì„ ì ìš©í•œë‹¤ê³  ìƒê°í•˜ë©´, ì ì ˆí•œ Augmentation ì„ ì •ê³¼ ì‚¬ìš©ê°€ëŠ¥í•œ resource ë‚´ì—ì„œ batch sizeì˜ ì„¤ì • ë“±ì´ ë§ì€ ì‹¤í—˜ì„ í•„ìš”ë¡œ í•  ê²ƒì…ë‹ˆë‹¤.</p>

<p>ì´ ë…¼ë¬¸ì—ì„  ì´ì „ ì—°êµ¬ë“¤ì— ë¹„í•´ batch sizeì™€ augmentationì— ëŒ€í•œ robustnessë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ, Image representation learning ì„±ëŠ¥ë„ ê°œì„ ëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, í•™ìŠµ ê³¼ì •ì—ì„œ negative sampleì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì ì´ ì´ì „ ì—°êµ¬ì™€ëŠ” ê°€ì¥ í° ì°¨ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ê°œì˜ neural networkì„ ì‚¬ìš©í•˜ê³  ì´ ì¤‘ í•˜ë‚˜ë¥¼ target networkë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ ì•„ì´ë””ì–´ ì…ë‹ˆë‹¤.</p>

<h2 id="methods">Methods</h2>
<p>ë…¼ë¬¸ì˜ motivationì€ ê°„ë‹¨í•œ ì‹¤í—˜ì—ì„œ ì¶œë°œí•©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post5/motive.jpg" alt="motive" /></p>

<ul>
  <li>step1 : randomly initialized networkë¥¼ freezeí•˜ê³  linear evaluationì„ ì§„í–‰í•©ë‹ˆë‹¤.<br />
-&gt; Acc 1.4%</li>
  <li>step2 : step1ì˜ networkì— MLPë¥¼ ë¶™ì—¬ prediction ê°’ì„ ì–»ìŠµë‹ˆë‹¤.</li>
  <li>step3 : step2ì—ì„œ ì–»ì€ prediction ê°’ì„ targetìœ¼ë¡œ í•˜ì—¬ step1ì²˜ëŸ¼ randomly initialized networkë¥¼ í•™ìŠµí•˜ê³  linear evaluationì„ ì§„í–‰í•©ë‹ˆë‹¤. <br />
-&gt; Acc 18.8%</li>
</ul>

<p>â€œë™ì¼ instanceì— ëŒ€í•´ ë‹¤ë¥¸ augmentationì„ ì ìš©í•œ ì´ë¯¸ì§€ëŠ” representationì´ ë™ì¼í•´ì•¼ í•œë‹¤.â€ ê°€ instance discrimination ë°©ì‹ì¸ contrastive learningì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ ì˜€ëŠ”ë°ìš”. ì´ ì•„ì´ë””ì–´ë§Œìœ¼ë¡œ í•™ìŠµ frameworkì„ êµ¬ì„±í•  ì‹œ, ëª¨ë¸ì€ inputê³¼ ë¬´ê´€í•˜ê²Œ ë™ì¼ representationì„ ì¶œë ¥í•˜ë„ë¡ í•™ìŠµë  ìˆ˜ ìˆê³  ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” â€˜collapsed representationsâ€™ì´ë¼ê³  ë§í•©ë‹ˆë‹¤. ì´ ë¬¸ì œê°€ ì´ì „ ì—°êµ¬ë“¤ì—ì„œ negative sampleì„ ì‚¬ìš©í•´ì•¼ë§Œ í–ˆë˜ ì´ìœ ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì—¬ê¸°ì„œ, í•™ìŠµì„ ì‹œì‘í•˜ê¸°ì „ randomly initialized networkì—ì„œ ì–»ì€ representationì„ ìƒê°í•´ë³´ë©´ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•œ representationì€ ì•„ë‹ˆê² ì§€ë§Œ, ì•„ì§ í•™ìŠµì„ ì‹œì‘í•˜ì§€ ì•Šì•˜ê¸°ì— collapsed ë¬¸ì œëŠ” ì—†ì„ ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì€ representationì˜ ì„±ëŠ¥ì´ step1ì—ì„œ êµ¬í•œ 1.4%ì˜€ìŠµë‹ˆë‹¤. ImageNet ë°ì´í„°ê°€ 1000ê°œì˜ classì„ì„ ê°ì•ˆí•˜ë©´ í•™ìŠµí•œ 1ê°œì˜ linear layerì—ì„œ ì–´ëŠì •ë„ì˜ í•™ìŠµì€ ì´ë£¨ì–´ì§„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.</p>

<p>step2~3ì˜ ê²°ê³¼ëŠ” step1ì—ì„œ ì–»ì€ ì˜ë¯¸ì—†ëŠ”(randomly initilized ì´ë¯€ë¡œ) representationì„ targetìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì–´ëŠ ì •ë„ì˜ ì˜ë¯¸ëŠ” ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ ê°œì¸ì ìœ¼ë¡œ ì¢€ ì‹ ê¸°í•œ ë¶€ë¶„ì´ì—ˆëŠ”ë°ìš”. í‹€ë¦° ë‹µìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì „í˜€ í•™ìŠµí•˜ì§€ ì•Šì€ ê²ƒë³´ë‹¤ëŠ” ë„ì›€ì´ ëœë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<p>ë…¼ë¬¸ì—ì„  ìœ„ ì‹¤í—˜ê²°ê³¼ë¥¼ ì‹œì‘ìœ¼ë¡œ representationì„ í•™ìŠµí•  online networkì™€ í•™ìŠµí•˜ëŠ” predictionì„ ì¶œë ¥í•  target networkë¥¼ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•˜ê²Œ ë©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post5/byol_thumbs.jpg" alt="byol_thumbs" /></p>

<ul>
  <li><img src="https://latex.codecogs.com/svg.latex?\; v, v' : " height="20" /> augmented image</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; f_{\theta}, f_{\xi} :" wdith="20" /> randomly initialized network</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; g_{\theta}, g_{\xi} :" wdith="20" /> projector</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; q_{\theta} : " height="13" /> predictor</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; sg(z'_{\xi}) : " height="20" /> stop gradient</li>
</ul>

<p>ì„œë¡œ ë‹¤ë¥¸ augmentationì„ ì ìš©í•œ ì´ë¯¸ì§€ì˜ featureë¥¼ ê°ê° ê³„ì‚°í•˜ì—¬ lossë¥¼ êµ¬í•˜ê³  downstream taskì—ëŠ” <img src="https://latex.codecogs.com/svg.latex?\; y_{\theta}" />ë§Œ ì‚¬ìš©ë˜ëŠ” ì ì€ SimCLRì™€ ìœ ì‚¬í•œ í˜•íƒœì´ê³ , onlineê³¼ target networkê°€ êµ¬ë¶„ëœë‹¤ëŠ” ì , onlineì—ë§Œ predictorê°€ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©í•œ ì ì´ í° ì°¨ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>target networkì˜ outputì€ stop gradientë¥¼ ì ìš©í•˜ê³  lossë¥¼ ê³„ì‚°í•˜ê¸°ì— online networkë§Œ í•™ìŠµ ê³¼ì • ì¤‘ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµë˜ê³ , target networkëŠ” online networkì˜ exponential moving averageë¡œ ë§¤ epoch ë§ˆë‹¤ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ MoCoì—ì„œ ì‚¬ìš©í•œ momentum encoderì™€ ìœ ì‚¬í•´ ë³´ì…ë‹ˆë‹¤.</p>

<p><img src="https://latex.codecogs.com/svg.latex?\; \xi \leftarrow \tau \xi + (1-\tau)\theta " height="25" /></p>

<p>Lossì™€ ì „ì²´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ ê°™ì€ ëª¨ì–‘ì´ê³ , lossì— ì‚¬ìš©ë˜ëŠ” <img src="https://latex.codecogs.com/svg.latex?\; q_{\theta}(z_{\theta}), z'_{\xi} " />ëŠ” l2 normalizationë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  onlineê³¼ target networkê°€ ë¹„ëŒ€ì¹­ì ì¸ êµ¬ì¡°ì´ê¸°ì— augmentationì„ ë°”ê¿”ì„œ ì ìš©í•œ lossë„ ë”í•˜ì—¬ ìµœì¢… lossê°€ ê³„ì‚°ë©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post5/loss.jpg" alt="loss" />
<img src="/assets/img/post5/algorithm.jpg" alt="algorithm" /></p>

<p>ì‚¬ìš©ëœ augmentation, architecture, optimization ë°©ì‹ì€ ëŒ€ë¶€ë¶„ SimCLRê³¼ ë¹„ìŠ·í•˜ë‹¤ê³  í•˜ê³ , ë…¼ë¬¸ Appendixì— ìì„¸íˆ ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>

<h3 id="augmentation">Augmentation</h3>
<ul>
  <li>random patch, resize, random horizontal flip</li>
  <li>color distortion</li>
  <li>Gaussian blur, solarization</li>
</ul>

<h3 id="architecture">Architecture</h3>
<ul>
  <li>ResNet(layer:50,101,152,200, width:1x~4x)</li>
  <li>projector, predictor : MLP(4096, BN, ReLU, 256), ë‘ ë²ˆì§¸ layer ë’¤ì— BNì´ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì ì´ SimCLRê³¼ ë‹¤ë¦„</li>
</ul>

<h3 id="optimization">Optimization</h3>
<ul>
  <li>LARS optimizer, cosine decay lr schedule, 1000 epochs</li>
  <li><img src="https://latex.codecogs.com/svg.latex?\; \tau_{base}=0.996  " height="20" />, <img src="/assets/img/post5/tau.jpg" alt="tau" /></li>
  <li>batch size = 4096, 512 Clout TPU v3 cores, 8 hours(ResNet50)</li>
</ul>

<h2 id="results">Results</h2>

<h2 id="conclusion">Conclusion</h2>

<p>&lt;â€”â€“ ì‘ì„±ì¤‘ â€”â€“&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/2006.07733â€‹">Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</a></p>

<p>etc : <br />
<a href="https://hoya012.github.io/blog/byol/">HOYA012â€™S RESEARCH BLOG : Bootstrap Your Own Latentï¼š A New Approach to Self-Supervised Learning ë¦¬ë·°</a><br />
<a href="https://2-chae.github.io/category/2.papers/26">https://2-chae.github.io/category/2.papers/26</a><br />
<a href="https://cool24151.tistory.com/85">https://cool24151.tistory.com/85</a> 
<a href="https://www.youtube.com/watch?v=BuyWUSPJicM">ë”¥ëŸ¬ë‹ë…¼ë¬¸ì½ê¸°ëª¨ì„ : ì¡°ìš©ë¯¼ - Bootstrap Your Own Latent(BYOL)</a></p>

:ET