I"s<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„  Computer Visionì— Attention ë˜ëŠ” Transformer êµ¬ì¡°ê°€ ì‚¬ìš©ëœ ëª‡ ê°€ì§€ ë…¼ë¬¸ì— ëŒ€í•´ ì •ë¦¬í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.</p>

<h2 id="show-attend-and-tell-neural-image-caption-generation-with-visual-attention">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</h2>
<p>ìœ„ëŠ” 2015ë…„ë„ì— ë‚˜ì˜¨ ë…¼ë¬¸ìœ¼ë¡œ, â€˜Attention is All You Needâ€™ë¥¼ í†µí•´ Transformer í˜•íƒœì˜ Attentionì´ ë“±ì¥í•˜ê¸° ì´ì „ì— Image Captioning taskì— Attentionì˜ ê°œë…ì„ ì‚¬ìš©í–ˆë˜ ë…¼ë¬¸ì…ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post8/fig1.jpg" width="500" /></p>

<p>Image Captioningì€ imageë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì´ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” taskë¥¼ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ CNNê³¼ RNNì´ ëª¨ë‘ ì‚¬ìš©ë˜ëŠ” êµ¬ì¡°ì´ê³ , CNNì˜ output feature mapì´ RNNì„ ê±°ì¹˜ë©´ì„œ sequenceë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ë•Œ, RNN êµ¬ì¡°ì— Attentionì„ ì ìš©í•œ ê²ƒì´ ì´ ë…¼ë¬¸ì˜ contributionì´ê³  ì´ë¥¼ í†µí•´ ìƒì„±ëœ ë‹¨ì–´ë³„ í•´ë‹¹ ì˜ì—­ì„ ì•„ë˜ì™€ ê°™ì´ ì´ë¯¸ì§€ì— í‘œì‹œí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.</p>

<p><img src="/assets/img/post8/fig2.jpg" width="500" /></p>

<p>ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p><img src="/assets/img/post8/fig3.jpg" width="700" /></p>

<ol>
  <li>ë¨¼ì €, pretraind CNN ëª¨ë¸ì„ í†µí•´ imageì˜ feature mapì„ ì–»ìŠµë‹ˆë‹¤.</li>
  <li>ì´ feature mapì„ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ LSTMì˜ ì²«ë²ˆ ì§¸ hidden stateì™€ cell stateë¥¼ êµ¬í•˜ê³ , attention weighted encodingì„ êµ¬í•©ë‹ˆë‹¤.</li>
  <li>í˜„ì¬ timestepì—ì„œ ê³„ì‚°ëœ attention weighted encodingê³¼ hidden ë° cell stateëŠ” ë‹¤ìŒ timestepì˜ wordë¥¼ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.</li>
</ol>

<p>ì´ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ attention êµ¬ì¡°ëŠ” hidden stateì™€ weighted encodingì„ element-wiseí•˜ê²Œ í•©í•œ ë‹¤ìŒ fc layerë¥¼ í†µê³¼í•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ë°, ì´ëŠ” keyì™€ queryì˜ dot productë¥¼ í†µí•´ similarityë¥¼ ê³„ì‚°í•˜ëŠ” í˜•íƒœì˜ ì˜ ì•Œë ¤ì§„ Transformerì™€ëŠ” ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h2>
<p><img src="/assets/img/post8/thumbs.jpg" width="700" /></p>

<p>ì´ ë…¼ë¬¸ì€ ìœ„ ë…¼ë¬¸ ì œëª©ë³´ë‹¨ ViT(Vision Transformer)ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë” ì˜ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ patch ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ ë§ˆì¹˜ sequenceì˜ word embeddingì´ ëª¨ë¸ì˜ inputìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì²˜ëŸ¼ ì´ë¯¸ì§€ì˜ patchë“¤ì´ ëª¨ë¸ì˜ inputìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”</p>

<h2 id="swin-transformer-hierarchical-vision-transformer-using-shifted-windows">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2>

<h2 id="pyramid-vision-transformer-a-versatile-backbone-for-dense-prediction-without-convolutions">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h2>

<h2 id="mlp-mixer-an-all-mlp-architecture-for-vision">MLP-Mixer: An all-MLP Architecture for Vision</h2>

<p>&lt;â€”â€”â€”â€”â€”-ì‘ì„± ì¤‘â€”â€”â€”â€”â€”-&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a><br />
<a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>  <br />
<a href="https://arxiv.org/abs/2103.14030â€‹">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>  <br />
<a href="https://arxiv.org/abs/2102.12122â€‹â€‹">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</a>   <br />
<a href="https://arxiv.org/abs/2105.01601â€‹â€‹">MLP-Mixer: An all-MLP Architecture for Vision</a></p>

<p>other :<br />
<a href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning">a-PyTorch-Tutorial-to-Image-Captioning</a></p>

:ET