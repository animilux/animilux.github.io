I"¢<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„  Computer Visionì— Attention ë˜ëŠ” Transformer êµ¬ì¡°ê°€ ì‚¬ìš©ëœ ëª‡ ê°€ì§€ ë…¼ë¬¸ì— ëŒ€í•´ ì •ë¦¬í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.</p>

<h2 id="show-attend-and-tell-neural-image-caption-generation-with-visual-attention">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</h2>
<p>ì´ ë…¼ë¬¸ì€ 2015ë…„ë„ì— ë‚˜ì˜¨ ë…¼</p>

<h2 id="an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h2>
<p><img src="/assets/img/post8/thumbs.jpg" alt="img1" /></p>

<h2 id="swin-transformer-hierarchical-vision-transformer-using-shifted-windows">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2>

<h2 id="pyramid-vision-transformer-a-versatile-backbone-for-dense-prediction-without-convolutions">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h2>

<h2 id="mlp-mixer-an-all-mlp-architecture-for-vision">MLP-Mixer: An all-MLP Architecture for Vision</h2>

<p>&lt;â€”â€”â€”â€”â€”-ì‘ì„± ì¤‘â€”â€”â€”â€”â€”-&gt;</p>

<h2 id="reference">Reference</h2>
<p>paper :<br />
<a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a><br />
<a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>  <br />
<a href="https://arxiv.org/abs/2103.14030â€‹">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>  <br />
<a href="https://arxiv.org/abs/2102.12122â€‹â€‹">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</a>   <br />
<a href="https://arxiv.org/abs/2105.01601â€‹â€‹">MLP-Mixer: An all-MLP Architecture for Vision</a></p>

<p>other :<br />
<a href="https://github.com/animilux/a-PyTorch-Tutorial-to-Image-Captioningâ€‹">a-PyTorch-Tutorial-to-Image-Captioning</a></p>

:ET